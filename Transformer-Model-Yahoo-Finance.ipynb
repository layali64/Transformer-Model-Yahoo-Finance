{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c673230",
   "metadata": {},
   "source": [
    "## Yahoo Finace Libaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "649dd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the stock symbol and date range\n",
    "stock_symbol = \"AAME\"\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Fetch historical stock data from Yahoo Finance\n",
    "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Reset index to make 'Date' a column again\n",
    "stock_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842a3d9",
   "metadata": {},
   "source": [
    "## Plot historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4a47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import plotly_express as px\n",
    "\n",
    "\n",
    "def plot_stock_data(data,title):\n",
    "    '''function for plotting stock data'''\n",
    "    plot = px.line(data, \n",
    "                        x=\"Date\", \n",
    "                        y=[\"Close\"], \n",
    "                        hover_name=\"Date\",\n",
    "                        line_shape=\"linear\",\n",
    "                        title=title) \n",
    "    return plot\n",
    "\n",
    "plot_stock_data(stock_data[-30:],'Airline') # for 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "188903f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data[['Close']].plot()\n",
    "plt.title(\"Closed Price Stock Market\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa807fa",
   "metadata": {},
   "source": [
    "## Outlier checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11787511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Date\" column to a datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column to analyze for outliers \n",
    "column_name = 'Open'\n",
    "\n",
    "# Calculate the IQR (Interquartile Range)\n",
    "Q1 = df[column_name].quantile(0.25)\n",
    "Q3 = df[column_name].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify and count outliers\n",
    "outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "num_outliers = len(outliers)\n",
    "\n",
    "# Display the number of outliers\n",
    "print(f'Number of outliers in {column_name}: {num_outliers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column to analyze for outliers \n",
    "column_name = 'Close'\n",
    "\n",
    "# Calculate the IQR (Interquartile Range)\n",
    "Q1 = df[column_name].quantile(0.25)\n",
    "Q3 = df[column_name].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify and count outliers\n",
    "outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "num_outliers = len(outliers)\n",
    "\n",
    "# Display the number of outliers\n",
    "print(f'Number of outliers in {column_name}: {num_outliers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef47470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Specify the column to visualize outliers \n",
    "column_name = 'Close'\n",
    "\n",
    "# Create a box plot to visualize outliers\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df[column_name], orient='v', width=0.3, palette='Set1')\n",
    "plt.title(f'Box Plot for {column_name} (with Outliers)')\n",
    "plt.ylabel(column_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c73931",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edaf8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = len(feature_columns)\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "num_attention_heads = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "# Define the number of time steps to use as input features\n",
    "num_time_steps = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12247242",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fcbf0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate Mean Absolute Percentage Error (MAPE)\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Define the stock symbol and date range\n",
    "stock_symbol = \"AAME\"\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Fetch historical stock data from Yahoo Finance\n",
    "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Reset index to make 'Date' a column again\n",
    "stock_data.reset_index(inplace=True)\n",
    "\n",
    "# Rename 'Adj Close' to 'Adjusted Close'\n",
    "stock_data.rename(columns={'Adj Close': 'Adjusted Close'}, inplace=True)\n",
    "\n",
    "# Use 'Date' as index\n",
    "stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize data\n",
    "columns_to_normalize = ['Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close']\n",
    "data_to_normalize = stock_data[columns_to_normalize].values\n",
    "\n",
    "# Convert the data to PyTorch tensor\n",
    "tensor_data = torch.tensor(data_to_normalize, dtype=torch.float32)\n",
    "\n",
    "# Calculate mean and standard deviation for each column\n",
    "mean = tensor_data.mean(dim=0)\n",
    "std = tensor_data.std(dim=0)\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = (tensor_data - mean) / std\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_normalized = pd.DataFrame(normalized_data.numpy(), columns=columns_to_normalize)\n",
    "\n",
    "# Add 'Date' column back to the DataFrame\n",
    "df_normalized['Date'] = stock_data.index\n",
    "data = df_normalized\n",
    "\n",
    "# Define the percentage of data for training, validation, and testing\n",
    "train_percent = 0.7\n",
    "val_percent = 0.15\n",
    "test_percent = 0.15\n",
    "\n",
    "# Calculate the sizes of the train, validation, and test sets\n",
    "train_size = int(train_percent * len(data))\n",
    "val_size = int(val_percent * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data = data.iloc[:train_size]\n",
    "val_data = data.iloc[train_size:train_size + val_size]\n",
    "test_data = data.iloc[train_size + val_size:]\n",
    "\n",
    "# Define the features(inputs) and target columns\n",
    "feature_columns = ['Low', 'Open', 'High']\n",
    "target_column = 'Close'\n",
    "\n",
    "# Extract features and target for each dataset\n",
    "train_features = train_data[feature_columns].values\n",
    "train_target = train_data[target_column].values\n",
    "\n",
    "val_features = val_data[feature_columns].values\n",
    "val_target = val_data[target_column].values\n",
    "\n",
    "test_features = test_data[feature_columns].values\n",
    "test_target = test_data[target_column].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "num_epochs= 100\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, cn) = self.lstm(x)  # Use both hidden state (hn) and cell state (cn)\n",
    "        x = self.fc_output(hn[-1, :, :])\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameters for LSTM\n",
    "hidden_size_lstm = 64\n",
    "num_layers_lstm = 2\n",
    "\n",
    "# Create the LSTM model instance\n",
    "model_lstm = LSTMModel(input_size, hidden_size_lstm, output_size, num_layers_lstm)\n",
    "\n",
    "# Define the loss function and optimizer for LSTM\n",
    "criterion= nn.MSELoss()\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store training, validation, and test losses for LSTM\n",
    "train_losses_lstm = []\n",
    "val_losses_lstm = []\n",
    "test_losses_lstm = []  # This list will store test loss for each epoch\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for LSTM\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    optimizer_lstm.zero_grad()\n",
    "\n",
    "    # Forward pass for LSTM\n",
    "    outputs_lstm = model_lstm(train_features.unsqueeze(1))\n",
    "    \n",
    "    # Calculate the loss for LSTM\n",
    "    loss_lstm = criterion(outputs_lstm, train_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagation and optimization for LSTM\n",
    "    loss_lstm.backward()\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    # Store the training loss for LSTM\n",
    "    train_losses_lstm.append(loss_lstm.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss (LSTM): {loss_lstm.item():.4f}')\n",
    "\n",
    "    # Validation loss for LSTM\n",
    "    model_lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs_lstm = model_lstm(val_features.unsqueeze(1))\n",
    "        val_loss_lstm = criterion(val_outputs_lstm, val_target.unsqueeze(1))\n",
    "    \n",
    "    # Store the validation loss for LSTM\n",
    "    val_losses_lstm.append(val_loss_lstm.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss (LSTM): {val_loss_lstm.item():.4f}')\n",
    "\n",
    "    # Test loss for LSTM\n",
    "    with torch.no_grad():\n",
    "        test_outputs_lstm = model_lstm(test_features.unsqueeze(1))\n",
    "        test_loss_lstm = criterion(test_outputs_lstm, test_target.unsqueeze(1))\n",
    "\n",
    "    # Store the test loss for each epoch for LSTM\n",
    "    test_losses_lstm.append(test_loss_lstm.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss (LSTM): {test_loss_lstm.item():.4f}')\n",
    "\n",
    "\n",
    "# Plotting the training and validation losses for LSTM\n",
    "plt.plot(range(1, num_epochs + 1), train_losses_lstm, label='Training Loss (LSTM)')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses_lstm, label='Validation Loss (LSTM)')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses_lstm, label='Test Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Epochs (LSTM)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Switch the LSTM model to evaluation mode\n",
    "model_lstm.eval()\n",
    "\n",
    "# Make predictions on the validation set using LSTM\n",
    "with torch.no_grad():\n",
    "    val_predictions_lstm = model_lstm(val_features.unsqueeze(1))\n",
    "\n",
    "# Convert predictions and targets back to numpy arrays for LSTM\n",
    "val_predictions_lstm = val_predictions_lstm.squeeze(1).numpy()\n",
    "\n",
    "# Calculate evaluation metrics for validation set using LSTM\n",
    "mae_val_lstm = mean_absolute_error(val_target.numpy(), val_predictions_lstm)\n",
    "mse_val_lstm = mean_squared_error(val_target.numpy(), val_predictions_lstm)\n",
    "rmse_val_lstm = np.sqrt(mse_val_lstm)\n",
    "mape_val_lstm = calculate_mape(val_target.numpy(), val_predictions_lstm)\n",
    "\n",
    "# Print the evaluation metrics for validation set using LSTM\n",
    "print('Validation Set Metrics (LSTM):')\n",
    "print(f'Mean Absolute Error (MAE): {mae_val_lstm:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse_val_lstm:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_val_lstm:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_val_lstm:.4f}')\n",
    "print()\n",
    "\n",
    "# Make predictions on the test set using LSTM\n",
    "with torch.no_grad():\n",
    "    test_predictions_lstm = model_lstm(test_features.unsqueeze(1))\n",
    "\n",
    "# Convert predictions and targets back to numpy arrays for LSTM\n",
    "test_predictions_lstm = test_predictions_lstm.squeeze(1).numpy()\n",
    "\n",
    "# Calculate evaluation metrics for test set using LSTM\n",
    "mae_test_lstm = mean_absolute_error(test_target.numpy(), test_predictions_lstm)\n",
    "mse_test_lstm = mean_squared_error(test_target.numpy(), test_predictions_lstm)\n",
    "rmse_test_lstm = np.sqrt(mse_test_lstm)\n",
    "mape_test_lstm = calculate_mape(test_target.numpy(), test_predictions_lstm)\n",
    "\n",
    "# Print the evaluation metrics for test set using LSTM\n",
    "print('Test Set Metrics (LSTM):')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test_lstm:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse_test_lstm:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test_lstm:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_test_lstm:.4f}')\n",
    "\n",
    "# Plotting the predicted values against true values for the test set using LSTM\n",
    "plt.plot(test_target.numpy(), label='True Values')\n",
    "plt.plot(test_predictions_lstm, label='Predicted Values (LSTM)', linestyle='dashed')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Close Price')\n",
    "plt.title('True vs Predicted Values for Test Set (LSTM)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0cedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9842929076853941\n"
     ]
    }
   ],
   "source": [
    "# Calculate R2\n",
    "r2 = r2_score(test_target, test_predictions_lstm)\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273afd80",
   "metadata": {},
   "source": [
    "## Prophet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6268809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "stock_ticker = 'AAPL'\n",
    "yfin = yf.Ticker(stock_ticker)\n",
    "data = yfin.history(period=\"max\")\n",
    "data = data[['Close']]\n",
    "print(data.tail())\n",
    "\n",
    "import yfinance as yf\n",
    "stock_ticker = 'AAPL'\n",
    "yfin = yf.Ticker(stock_ticker)\n",
    "data = yfin.history(period=\"max\")\n",
    "data = data[['Close']]\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data = data.rename({'Date': 'ds', 'Close': 'y'}, axis='columns')\n",
    "print(data.tail())\n",
    "\n",
    "\n",
    "data['ds'] = data['ds'].dt.tz_localize(None)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6060fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch historical stock data\n",
    "stock_symbol = \"AAL\"\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Download stock data from Yahoo Finance\n",
    "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Prepare data for Prophet\n",
    "prophet_data = stock_data.reset_index()[['Date', 'Close']]\n",
    "prophet_data.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Create and fit the Prophet model\n",
    "model = Prophet()\n",
    "model.fit(prophet_data)\n",
    "\n",
    "# Create a dataframe for future dates (1 year)\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Plot the forecast\n",
    "fig = model.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7638d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95989f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dfd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = m.plot(forecast)\n",
    "figure2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf919831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "plot_plotly(m, forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef55b5",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ba42cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate Mean Absolute Percentage Error (MAPE)\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Define the stock symbol and date range\n",
    "stock_symbol = \"AAME\"\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Fetch historical stock data from Yahoo Finance\n",
    "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Reset index to make 'Date' a column again\n",
    "stock_data.reset_index(inplace=True)\n",
    "\n",
    "# Rename 'Adj Close' to 'Adjusted Close'\n",
    "stock_data.rename(columns={'Adj Close': 'Adjusted Close'}, inplace=True)\n",
    "\n",
    "# Use 'Date' as index\n",
    "stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize data\n",
    "columns_to_normalize = ['Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close']\n",
    "data_to_normalize = stock_data[columns_to_normalize].values\n",
    "\n",
    "# Convert the data to PyTorch tensor\n",
    "tensor_data = torch.tensor(data_to_normalize, dtype=torch.float32)\n",
    "\n",
    "# Calculate mean and standard deviation for each column\n",
    "mean = tensor_data.mean(dim=0)\n",
    "std = tensor_data.std(dim=0)\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = (tensor_data - mean) / std\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_normalized = pd.DataFrame(normalized_data.numpy(), columns=columns_to_normalize)\n",
    "\n",
    "# Add 'Date' column back to the DataFrame\n",
    "df_normalized['Date'] = stock_data.index\n",
    "data = df_normalized\n",
    "\n",
    "# Define the percentage of data for training, validation, and testing\n",
    "train_percent = 0.7\n",
    "val_percent = 0.15\n",
    "test_percent = 0.15\n",
    "\n",
    "# Calculate the sizes of the train, validation, and test sets\n",
    "train_size = int(train_percent * len(data))\n",
    "val_size = int(val_percent * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data = data.iloc[:train_size]\n",
    "val_data = data.iloc[train_size:train_size + val_size]\n",
    "test_data = data.iloc[train_size + val_size:]\n",
    "\n",
    "# Define the features and target columns\n",
    "feature_columns = ['Low', 'Open', 'High']\n",
    "target_column = 'Close'\n",
    "\n",
    "# Extract features and target for each dataset\n",
    "train_features = train_data[feature_columns].values\n",
    "train_target = train_data[target_column].values\n",
    "\n",
    "val_features = val_data[feature_columns].values\n",
    "val_target = val_data[target_column].values\n",
    "\n",
    "test_features = test_data[feature_columns].values\n",
    "test_target = test_data[target_column].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "# Define a simple Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, num_attention_heads):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_attention_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Adjust input shape for the transformer\n",
    "        x = self.transformer(x, x)  # Set source and target as the same data\n",
    "        x = x.permute(1, 0, 2)  # Restore the original shape\n",
    "        x = self.fc_output(x[:, -1, :])  # Use the last layer's output for prediction\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = len(feature_columns)\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "num_attention_heads = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Create the model instance\n",
    "model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_attention_heads)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store training, validation, and test losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = [] \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(train_features.unsqueeze(1))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, train_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store the training loss\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_features.unsqueeze(1))\n",
    "        val_loss = criterion(val_outputs, val_target.unsqueeze(1))\n",
    "    \n",
    "    # Store the validation loss\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Test loss\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features.unsqueeze(1))\n",
    "        test_loss = criterion(test_outputs, test_target.unsqueeze(1))\n",
    "\n",
    "    # Store the test loss for each epoch\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the test loss for the last epoch\n",
    "print(f'Final Test Loss: {test_losses[-1]:.4f}')\n",
    "print()\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(val_features.unsqueeze(1))\n",
    "\n",
    "# Convert predictions and targets back to numpy arrays\n",
    "val_predictions = val_predictions.squeeze(1).numpy()\n",
    "val_target_numpy = val_target.numpy()\n",
    "\n",
    "# Calculate evaluation metrics for validation set\n",
    "mae_val = mean_absolute_error(val_target_numpy, val_predictions)\n",
    "mse_val = mean_squared_error(val_target_numpy, val_predictions)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "mape_val = calculate_mape(val_target_numpy, val_predictions)\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print('Validation Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_val:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse_val:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_val:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_val:.4f}')\n",
    "print()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(test_features.unsqueeze(1))\n",
    "\n",
    "# Convert predictions and targets back to numpy arrays\n",
    "test_predictions = test_predictions.squeeze(1).numpy()\n",
    "test_target_numpy = test_target.numpy()\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "mae_test = mean_absolute_error(test_target_numpy, test_predictions)\n",
    "mse_test = mean_squared_error(test_target_numpy, test_predictions)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mape_test = calculate_mape(test_target_numpy, test_predictions)\n",
    "\n",
    "# Print the evaluation metrics for test set\n",
    "print('Test Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse_test:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape_test:.4f}')\n",
    "\n",
    "# Plotting the predicted values against true values for the test set\n",
    "plt.plot(test_target_numpy, label='True Values')\n",
    "plt.plot(test_predictions, label='Predicted Values', linestyle='dashed')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Close Price')\n",
    "plt.title('True vs Predicted Values for Test Set')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b89b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
